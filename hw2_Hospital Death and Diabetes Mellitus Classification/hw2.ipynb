{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NzLuYS6pwUv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gCDMiNJVDAS"
      },
      "source": [
        "# **HW 2: Classification**\n",
        "In *homework 2*, you need to finish:\n",
        "\n",
        "1.  Basic Part:\n",
        "\n",
        "> *   Step 1: Load the input Data\n",
        "> *   Step 2: Implement Naive Bayesian classifier\n",
        "> *   Step 3: Build the classifier and check the correctness of Table building\n",
        "> *   Step 4: Split Data\n",
        "> *   Step 5: Make prediction and perform evaluation\n",
        "> *   Step 6: Generate results\n",
        "\n",
        "2.  Advanced Part:\n",
        "\n",
        "> *   Step 1: Load the input Data\n",
        "> *   Step 2: Implement Gaussian Naive Bayesian classifier\n",
        "> *   Step 3: Build the classifier and check the correctness of Table building\n",
        "> *   Step 4 Improve the classifier for Ranking\n",
        "> *   Step 5: Make prediction and perform evaluation\n",
        "> *   Step 6: Generate results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iSLeN98WAGM"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the Naive Bayesian classifier:\n",
        "- input features: ***9 physiological features***\n",
        "- output prediction: ***hospital_death***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGnqdEnXT20"
      },
      "source": [
        "## *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ROPDPsjdXpSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLeI_t2PAEPe"
      },
      "source": [
        "## Global attributes\n",
        "> You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBWtZJPLAIdJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjMH9mAIXnAU"
      },
      "source": [
        "## Step 1: Load the input Data\n",
        "Load the input file **hw2_basic_training.csv**\n",
        "\n",
        "> Note: please don't change the input var name ***training_df, testing_df, X, and y***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yDh8Ah9RY3z0"
      },
      "outputs": [],
      "source": [
        "# TODO: modify your file path\n",
        "training_df = pd.read_csv('hw2_basic_training.csv')\n",
        "testing_df = pd.read_csv('hw2_basic_testing.csv')\n",
        "y = training_df['hospital_death']\n",
        "X = training_df.drop('hospital_death', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr06keeMWKKu"
      },
      "source": [
        "you can take a look at the input feature & ground truth format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0pbarO8ja0F"
      },
      "outputs": [],
      "source": [
        "X[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxvHkm1XjbUh"
      },
      "outputs": [],
      "source": [
        "y[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuKwSE0ohxt"
      },
      "source": [
        "## Step 2: Implement Naive Bayesian classifier\n",
        "In this part, you need to implement the Naive Bayesian classifier. Since the data is categorical, you can refer to the L3-Bayesian Classifier course slides, p.12~16. The Bayes' theorem is as follows:\n",
        "\n",
        "$$P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$$\n",
        "\n",
        "We know that taking the logarithm of a series of multiplications can be transformed into a series of additions, making it easier to calculate. So, we can formulate it as follows by taking the logarithm of both sides:\n",
        "\n",
        "$$\\log(P(C|X)) = \\log(P(X|C)) + \\log(P(C)) - \\log(P(X))$$\n",
        "\n",
        "The term $\\log(P(X))$ is a normalization constant that ensures the probabilities sum to 1 across all classes and is the same for all classes. Since it's constant during prediction for all classes, it doesn't affect class comparison. Therefore, in practice, you don't need to compute or include $\\log(P(X))$ explicitly when comparing classes. Instead, you can focus on the relative values of the posterior probabilities.\n",
        "\n",
        "So the **final equation** for implementation will be:\n",
        "$$\\log(P(C|X)) = \\log(P(X|C)) + \\log(P(C))$$\n",
        "\n",
        "However, this equation may still encounter issues if the likelihood $P(X|C)$ equals 0, leading to an undefined $\\log(P(X|C))$. To handle this exception, a simple way to avoid the issue is to assume the addition of one new record to the table to calculate the likelihood:\n",
        "\n",
        "_likelihood = 1 / (len(self.feature_probs_table[c][feature]) + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iRb5mruK9w-p"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def build_table(self, X, y):\n",
        "        # classes for ground truth: there are only negative(0) and positive(1) for y (hospital_death)\n",
        "        self.classes = np.unique(y)\n",
        "        # record prior for two classes\n",
        "        self.class_priors = {}\n",
        "        # **feature_probs_table** is a 3D dictionary table:\n",
        "        # structure: [class]    [feature]           [value] = probs\n",
        "        # example:   [0]        ['gcs_eyes_apache'] [3]     = # of (hospital_death=0 && gcs_eyes_apache=3) / # of (hospital_death=0)\n",
        "        # for more usage of python dict, you can refer to the link: https://www.w3schools.com/python/python_dictionaries.asp\n",
        "        self.feature_probs_table = {}\n",
        "\n",
        "        # consider negative(0) and positive(1) separately\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_priors[c] = len(X_c) / len(X) # TODO: calculate the prior\n",
        "            self.feature_probs_table[c] = {}\n",
        "\n",
        "            for feature in X.columns:\n",
        "                self.feature_probs_table[c][feature] = {}\n",
        "                for value in np.unique(X_c[feature]):\n",
        "                    value_count = len(X_c[X_c[feature] == value]) # TODO: Calculate the count of data points with the current feature value and the current class\n",
        "                    total_count = len(X_c[feature]) # TODO: Calculate the total count of data points with the current feature within the current class\n",
        "                    self.feature_probs_table[c][feature][value] = value_count / total_count # TODO: Calculate and store the conditional probability of the feature value given the class\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X.to_dict(orient='records')]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        log_posteriors = []\n",
        "\n",
        "        # this for loop implement: log(posteior) = log(prior) + log(likelihood)\n",
        "        for c in self.classes:\n",
        "            log_prior = np.log(self.class_priors[c])\n",
        "            log_likelihood = 0\n",
        "            for feature, value in x.items():\n",
        "                if value in self.feature_probs_table[c][feature]:\n",
        "                    # you can look up the table for the likelihood\n",
        "                    _likelihood = self.feature_probs_table[c][feature][value] # TODO: calculate likelihood by the self.feature_probs_table\n",
        "                else:\n",
        "                    # to fix issue of some cases not appear in the table\n",
        "                    _likelihood = 1 / (len(self.feature_probs_table[c][feature]) + 1)\n",
        "                log_likelihood += np.log(_likelihood)\n",
        "            log_posterior = log_prior + log_likelihood\n",
        "            log_posteriors.append((c, log_posterior))\n",
        "        return max(log_posteriors, key=lambda x: x[1])[0] # TODO: Return the class with the highest logarithm of posterior probability as the predicted class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m_uQfkmTgyC"
      },
      "source": [
        "## Step 3: Build the classifier and check the correctness of Table building\n",
        "You can easily build an instance of your classifier and then create the table.\n",
        "\n",
        "To check whether you have correctly built the table of the Naive Bayesian classifier, there is an example for you to ensure that your implementation is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sc1CTjeYTfp0"
      },
      "outputs": [],
      "source": [
        "# Create and build the dictionary table for Naive Bayes classifier\n",
        "nb_classifier = NaiveBayes()\n",
        "nb_classifier.build_table(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-f_QvJKZegX"
      },
      "source": [
        "And you also need to output the dictionary variable ***feature_probs_table*** as a pickle file, which will be examined for correctness.\n",
        "> Note: Since this is for checking the implementation of the build_table method, please ensure that the input for your table building, ***X and y,*** is taken from the provided hw2_basic_training.csv file ***BEFORE*** splitting the dataset into training and validation sets.\n",
        "\n",
        "> Hint: Two values for you to check the implementation correctness:\n",
        "\n",
        "> `nb_classifier.feature_probs_table[0]['gcs_eyes_apache'][3]` is\n",
        "0.15299\n",
        "\n",
        "> `nb_classifier.feature_probs_table[1]['gcs_eyes_apache'][3]` is\n",
        "0.15896"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX4vtks6qVaK",
        "outputId": "bd964cf8-12d7-4947-8268-14c54d414f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pass\n"
          ]
        }
      ],
      "source": [
        "if round(nb_classifier.feature_probs_table[0]['gcs_eyes_apache'][3], 5) == 0.15299 and \\\n",
        "   round(nb_classifier.feature_probs_table[1]['gcs_eyes_apache'][3], 5) == 0.15896:\n",
        "    print('pass')\n",
        "else:\n",
        "    print('fail')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uia7kWBagdE"
      },
      "outputs": [],
      "source": [
        "# TODO: change your path to save the pickle file\n",
        "pickle_file_path = 'hw2_basic_table'\n",
        "with open(pickle_file_path, 'wb') as table_file:\n",
        "    pickle.dump(nb_classifier.feature_probs_table, table_file)\n",
        "    table_file.close()\n",
        "\n",
        "'''\n",
        "# Print the pickle file\n",
        "from pprint import pprint\n",
        "with open(pickle_file_path, 'rb') as table_file:\n",
        "    loaded_data = pickle.load(table_file)\n",
        "pprint(loaded_data)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI3rKUuzZUb4"
      },
      "source": [
        "## Step 4: Split Data\n",
        "Split the data in *X, and y* into the training dataset and validation dataset.\n",
        "> Note: You can use what you have implemented in hw1.\n",
        "\n",
        "Since many students may not understand the meaning of a validation set, let's provide more explanation:\n",
        "\n",
        "The purpose of a validation set is to determine whether our model is overfitting the training data.\n",
        "- Underfitting: If the performance on the training set is poor (e.g., you haven't prepared enough for exam 1).\n",
        "- Overfitting: If the performance on the training set is high, but the performance on the validation set is poor. (e.g., if you've focused solely on practicing with \"past exam papers\" (考古題) for exam 1, you might answer those questions correctly but struggle with new, unfamiliar questions during the actual exam).\n",
        "\n",
        "If we achieve good performance on both the training set and the validation set, it indicates that the model has not only learned from the training data but also has the ability to make accurate inferences on unseen data.\n",
        "\n",
        "![](https://hackmd.io/_uploads/SJLptEZWT.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5G-VlT4VkYY"
      },
      "source": [
        "Please split the dataset into training set and validation set\n",
        "\n",
        "> Note: The purpose of ***random_state*** is to ensure that you can reproduce the results each time you split your dataset. This is often helpful for debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YuY2DkQAXJh3"
      },
      "outputs": [],
      "source": [
        "def train_val_split(X, y, val_size=0.2, random_state=42):\n",
        "    # TODO: implement your own train_val_split\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    shuffled_indices = np.random.permutation(len(X))\n",
        "    val_set_size = int(len(X) * val_size)\n",
        "\n",
        "    val_indices = shuffled_indices[:val_set_size]\n",
        "    train_indices = shuffled_indices[val_set_size:]\n",
        "\n",
        "    X_train = X.iloc[train_indices]\n",
        "    y_train = y.iloc[train_indices]\n",
        "    X_val = X.iloc[val_indices]\n",
        "    y_val = y.iloc[val_indices]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wonY4SJdVjhF"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the data into training and validation sets\n",
        "# Note: please follow template for the format of return variables\n",
        "X_train, X_val, y_train, y_val = train_val_split(X, y) # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3BwyXwu0dOq"
      },
      "source": [
        "## Step 5: Make predictions and perform evaluation\n",
        "The method we will evaluate the performance of the Bayesian classifier is ***F1-socre***:\n",
        "\n",
        "$$\\text{precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
        "\n",
        "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
        "\n",
        "$$F1\\text{-}score = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
        "\n",
        "![](https://hackmd.io/_uploads/B1tfT9UWp.png)\n",
        "\n",
        "Since the number of ground truth ***hospital_death*** where the outcome is positive is much less than the number of negative outcomes, we should focus on the f1-score of the positive class.\n",
        "\n",
        "You need to implement the f1-score function to evaluate the performance of the Naive Bayesian classifier.\n",
        "\n",
        "> Note: You should test your classifier by evaluating it on the training set and the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLs0Y-veokCe"
      },
      "outputs": [],
      "source": [
        "def cal_f1_score(y_true, y_pred):\n",
        "    # Calculate True Positives, False Positives, False Negatives\n",
        "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
        "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1) # TODO: calculate the false positive\n",
        "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0) # TODO: calculate the false negative\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if tp + fp != 0 else 0 # TDOO: calculate the precision\n",
        "    recall = tp / (tp + fn) if tp + fn != 0 else 0 # TODO: calculate the recall\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0 # TODO: calculate the f1-score\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRtCpU6YtsBK"
      },
      "outputs": [],
      "source": [
        "# TODO: build table on the training set\n",
        "nb_classifier.build_table(X_train, y_train)\n",
        "\n",
        "# TODO: Make predictions on training set and calculate the f1-score\n",
        "train_predictions = nb_classifier.predict(X_train)\n",
        "train_f1_score = cal_f1_score(y_train, train_predictions)\n",
        "\n",
        "# TODO: Make predictions on validation set and calculate the f1-score\n",
        "val_predictions = nb_classifier.predict(X_val)\n",
        "val_f1_score = cal_f1_score(y_val, val_predictions)\n",
        "\n",
        "'''\n",
        "# Print the results\n",
        "print(f\"Training F1-score: {train_f1_score:.4f}\")\n",
        "print(f\"Validation F1-score: {val_f1_score:.4f}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgH6lNeJ_IBY"
      },
      "source": [
        "## Step 6: Generate result\n",
        "> Note: Please follow the format mension in the slides, the can only change the path for saving your code down below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qbeO-EFC_I4K"
      },
      "outputs": [],
      "source": [
        "predictions = nb_classifier.predict(testing_df) # TODO: predict on the testing_df\n",
        "\n",
        "# TODO: Specify the CSV file path\n",
        "csv_file_path = 'hw2_basic_prediction.csv'\n",
        "\n",
        "# Write the predictions to the CSV file\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # *** 10/21 update: header name ***\n",
        "    writer.writerow(['hospital_death'])\n",
        "    for prediction in predictions:\n",
        "        writer.writerow([prediction])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_yxRh0tyyHR"
      },
      "source": [
        "# **1. Advanced Part (40%)**\n",
        "In advanced part, you need to implement the Gaussian Bayesian classifier:\n",
        "- input features: ***3 physiological features***\n",
        "- output prediction: ***diabetes_mellitus***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIXO1GcSAUtG"
      },
      "source": [
        "## Global attributes\n",
        "> You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrJu6pEAUQQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDZpQtgTzBAz"
      },
      "source": [
        "## Step 1: Load the input Data\n",
        "Load the input file **hw2_advanced_training.csv**\n",
        "> Note: please don't change the input var name ***training_df, testing_df, X, and y***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EICIDlzJy7NB"
      },
      "outputs": [],
      "source": [
        "# TODO: modify your file path\n",
        "training_df = pd.read_csv('hw2_advanced_training.csv')\n",
        "testing_df = pd.read_csv('hw2_advanced_testing.csv')\n",
        "y = training_df['diabetes_mellitus']\n",
        "X = training_df.drop('diabetes_mellitus', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpwc5bwyDo_6"
      },
      "outputs": [],
      "source": [
        "X[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQG0xcKaDtDy"
      },
      "outputs": [],
      "source": [
        "y[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTzwktNcQ0gV"
      },
      "source": [
        "you can check whether the standardization works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKKgJmzVzhy_"
      },
      "source": [
        "## Step 2: Implement Gaussian Naive Bayesian classifier\n",
        "In this part, you need to implement the Gaussian Naive Bayesian classifier.\n",
        "\n",
        "The main difference between Naive Bayesian and Gaussian Naive Bayesian is the likelihood part. For Gaussian NB, we can use the probability density function (PDF) of the ***Gaussian distribution*** (also known as the Normal distribution):\n",
        "\n",
        "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp({-\\frac{(x - \\mu)^2}{2\\sigma^2}})$$\n",
        "\n",
        "The reason we need to use Gaussian is that when the data type is continuous numbers instead of discrete numbers, we can't build a table by just counting all the possible cases. However, we can assume the data distribution follows a Gaussian (or Normal) distribution by calculating its mean and variance. Then, we can approximate the values, even if some records don't appear in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NbKCkAvLzwHK"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayesian:\n",
        "    def build_table(self, X, y):\n",
        "        # classes for ground truth: there are only negative(0) and positive(1) for y (hospital_death)\n",
        "        self.classes = np.unique(y)\n",
        "        # record prior for two classes\n",
        "        self.class_priors = {}\n",
        "        # **feature_mean_var_table** is a 3D dictionary table:\n",
        "        # structure: [class]    [feature]           ['mean'] = mean\n",
        "        # structure: [class]    [feature]           ['var']  = var\n",
        "        # example:   [0]        ['gcs_eyes_apache'] ['mean'] = mean for feature='gcs_eyes_apache' when hospital_death=0\n",
        "        # example:   [0]        ['gcs_eyes_apache'] ['var']  = var for feature='gcs_eyes_apache' when hospital_death=0\n",
        "        self.feature_mean_var_table = {}\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_priors[c] = len(X_c) / len(X) # TODO: calculate prior\n",
        "            self.feature_mean_var_table[c] = {}\n",
        "            for feature in X.columns:\n",
        "                self.feature_mean_var_table[c][feature] = {}\n",
        "                # Calculate mean and var for each feature\n",
        "                self.feature_mean_var_table[c][feature]['mean'] = X_c[feature].mean() # TDOO: calculate the mean\n",
        "                # *** 10/19 note: make sure that if you call numpy.var, the ddof should set as 1 ***\n",
        "                self.feature_mean_var_table[c][feature]['var'] = X_c[feature].var(ddof=1)  # TODO: calculate the var\n",
        "\n",
        "    def _calculate_likelihood(self, x, mean, var):\n",
        "        return (1/np.sqrt(2*np.pi*var)) * np.exp((-(x-mean)**2) / (2*var)) # TODO: calculate the Gaussian (normal) distribution pdf function as likelihoo\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X.to_dict(orient='records')]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        log_posteriors = []\n",
        "        # this for loop implement: log(posteior) = log(prior) + log(likelihood)\n",
        "        for c in self.classes:\n",
        "            log_prior = np.log(self.class_priors[c])\n",
        "            log_likelihood = 0\n",
        "            for feature, value in x.items():\n",
        "                mean = self.feature_mean_var_table[c][feature]['mean']\n",
        "                var = self.feature_mean_var_table[c][feature]['var']\n",
        "                log_likelihood += np.log(self._calculate_likelihood(value, mean, var)) # TODO: calculate the log likelihood\n",
        "            log_posterior = log_prior + log_likelihood\n",
        "            log_posteriors.append((c, log_posterior))\n",
        "        return max(log_posteriors, key=lambda x: x[1])[0] # TODO: Return the class with the highest logarithm of posterior probability as the predicted class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGcgNBK9gs2"
      },
      "source": [
        "## Step 3: Build the classifier and check the correctness of Table building\n",
        "You can easily build an instance of your classifier and then create the table.\n",
        "\n",
        "To check whether you have correctly built the table of the ***Gaussian Naive Bayesian classifier***, there is an example for you to ensure that your implementation is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "52cFQayn8FGQ"
      },
      "outputs": [],
      "source": [
        "# Initialize and build_table the model\n",
        "gnb_classifier = GaussianNaiveBayesian()\n",
        "gnb_classifier.build_table(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC-bk7gT-2Ts"
      },
      "source": [
        "And you also need to output the dictionary variable ***feature_mean_var_table*** as a pickle file, which will be examined for correctness.\n",
        "> Note: Since this is for checking the implementation of the build_table method, please ensure that the input for your table building, ***X and y,*** is taken from the provided hw2_advanced_training.csv file ***BEFORE*** splitting the dataset into training and validation sets.\n",
        "\n",
        "> Hint: Two values for you to check the implementation correctness:\n",
        "\n",
        "\n",
        "> `gnb_classifier.feature_mean_var_table[0]['bmi']['mean']` is\n",
        "28.61544\n",
        "\n",
        "> `gnb_classifier.feature_mean_var_table[0]['bmi']['var']` is\n",
        "63.57263"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYzPyNIEU0l",
        "outputId": "8a18db85-5e7b-44fa-8a7b-46ef75603346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pass\n"
          ]
        }
      ],
      "source": [
        "# *** 10/18 update: the value of mean and var***\n",
        "if round(gnb_classifier.feature_mean_var_table[0]['bmi']['mean'], 5) == 28.61544 and \\\n",
        "   round(gnb_classifier.feature_mean_var_table[0]['bmi']['var'], 5) == 63.57263:\n",
        "    print('pass')\n",
        "else:\n",
        "    print('fail')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zDJLzdxqEUXN"
      },
      "outputs": [],
      "source": [
        "# TODO: change your path to save the pickle file\n",
        "pickle_file_path = 'hw2_advanced_table'\n",
        "with open(pickle_file_path, 'wb') as table_file:\n",
        "    pickle.dump(gnb_classifier.feature_mean_var_table, table_file)\n",
        "    table_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV_-5sKla5Gi"
      },
      "source": [
        "## Step 4 Improve the classifier for Ranking 15%:\n",
        "\n",
        "To make your model have better performance, you can try different ways to modify your model.\n",
        "\n",
        "> hints (**you don't need to follow the hints**):\n",
        "\n",
        "1. You can deal with the **outliers**\n",
        "2. You can try first **converting real numbers into discrete values** and then using Naive Bayesian for classification.\n",
        "3. You can try **def a new class for giving the prior a different weight** for decision-making.\n",
        "4. Anything you want to try based on Bayesian.\n",
        "\n",
        "> Note: You need to consider what kind of operations should also be performed on the testing_df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ZTX6Oca-3G"
      },
      "outputs": [],
      "source": [
        "# TODO: you can try the hints written above to get higher ranking score\n",
        "\n",
        "training_df = pd.read_csv('hw2_advanced_training.csv') # TODO: modify your file path\n",
        "testing_df = pd.read_csv('hw2_advanced_testing.csv') # TODO: modify your file path\n",
        "# Note: **you can change the order of following steps if you want**\n",
        "# Note: **BUT, please make sure you have saved 'hw2_advanced_table' for submission BEFORE making the following improvements.**\n",
        "\n",
        "# ... # hints(optional): deal with outliers\n",
        "def remove_outliers(df):\n",
        "  features = df.columns[df.columns != 'diabetes_mellitus']  # Exclude the 'diabetes_mellitus' column\n",
        "  Q1 = df[features].quantile(0.25)\n",
        "  Q3 = df[features].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  df_clean = df[~((df[features] < (Q1 - 1.5 * IQR)) | (df[features] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "  return df_clean\n",
        "\n",
        "training_df = remove_outliers(training_df)\n",
        "y = training_df['diabetes_mellitus']\n",
        "X = training_df.drop('diabetes_mellitus', axis=1)\n",
        "\n",
        "# ... # hints(optional): converting real numbers into discrete values\n",
        "# ... # hints(optional): def a new class for giving the prior a different weight\n",
        "# ... # hints: Split the data into training and validation sets\n",
        "def train_val_split(X, y, val_size=0.2, random_state=42):\n",
        "  np.random.seed(random_state)\n",
        "  shuffled_indices = np.random.permutation(len(X))\n",
        "  val_set_size = int(len(X) * val_size)\n",
        "\n",
        "  val_indices = shuffled_indices[:val_set_size]\n",
        "  train_indices = shuffled_indices[val_set_size:]\n",
        "\n",
        "  X_train = X.iloc[train_indices]\n",
        "  y_train = y.iloc[train_indices]\n",
        "  X_val = X.iloc[val_indices]\n",
        "  y_val = y.iloc[val_indices]\n",
        "\n",
        "  return X_train, X_val, y_train, y_val\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_val_split(X, y)\n",
        "\n",
        "# ... # anything you want to try based on Bayesian\n",
        "\n",
        "def cal_f1_score(y_true, y_pred):\n",
        "    # Calculate True Positives, False Positives, False Negatives\n",
        "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
        "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1) # TODO: calculate the false positive\n",
        "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0) # TODO: calculate the false negative\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if tp + fp != 0 else 0 # TDOO: calculate the precision\n",
        "    recall = tp / (tp + fn) if tp + fn != 0 else 0 # TODO: calculate the recall\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0 # TODO: calculate the f1-score\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtFVxggz7I4"
      },
      "source": [
        "## Step 5: Make predictions and perform evaluation\n",
        "You should test your model by evaluating the training set and validation set using the ***cal_f1_score*** function you implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "415sqX0D_jFY"
      },
      "outputs": [],
      "source": [
        "# TODO: build table on the training set\n",
        "gnb_classifier = GaussianNaiveBayesian()\n",
        "gnb_classifier.build_table(X_train, y_train)\n",
        "\n",
        "# TODO: Make predictions on training set and calculate the f1-score\n",
        "train_predictions = gnb_classifier.predict(X_train)\n",
        "train_f1_score = cal_f1_score(y_train, train_predictions)\n",
        "\n",
        "# TODO: Make predictions on validation set and calculate the f1-score\n",
        "val_predictions = gnb_classifier.predict(X_val)\n",
        "val_f1_score = cal_f1_score(y_val, val_predictions)\n",
        "\n",
        "'''\n",
        "# Print the results\n",
        "print(f\"Training F1-score: {train_f1_score:.4f}\")\n",
        "print(f\"Validation F1-score: {val_f1_score:.4f}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJWF-tfA_Q9d"
      },
      "source": [
        "## Step 6: Generate result\n",
        "> Note: Please follow the format mentioned in the slides. You can only change the path for saving your code down below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZX6FV9tRK6Fp"
      },
      "outputs": [],
      "source": [
        "predictions = gnb_classifier.predict(testing_df) # TODO: predict on the testing_df\n",
        "\n",
        "# TODO: Specify the CSV file path\n",
        "csv_file_path = 'hw2_advanced_prediction.csv'\n",
        "\n",
        "# Write the predictions to the CSV file\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # *** 10/21 update: header name ***\n",
        "    writer.writerow(['diabetes_mellitus'])\n",
        "    for prediction in predictions:\n",
        "        writer.writerow([prediction])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17MKZrvgSQMK"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw2_report.pdf**\n",
        "\n",
        "* Briefly describe why we take log when implement the Bayesian classifier? (1%)\n",
        "* Briefly describe the difference between Naïve Bayesian and Gaussian Naïve Bayesian classifier? (1%)\n",
        "* Briefly describe the difficulty you encountered (1%)\n",
        "* Summarize how you solve the difficulty and your reflections (2%)\n",
        "* **No more than one page**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4P5yTr4Sa3k"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw2.ipynb**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I24lVr726Y2h"
      },
      "source": [
        "# Submission:\n",
        "1. hw2_basic_prediction.csv\n",
        "2. hw2_basic_table: **make sure you build_table BEFORE split train_val set, and pass the given example**\n",
        "3. hw2_advanced_prediction.csv\n",
        "4. hw2_advanced_table: **make sure you build_table BEFORE split train_val set or pre-processing, and pass the given example**\n",
        "5. hw2.ipynb\n",
        "6. hw2_report.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}