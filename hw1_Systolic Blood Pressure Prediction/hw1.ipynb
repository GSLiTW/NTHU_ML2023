{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V06iW39mNy01",
        "outputId": "5fe9bdb3-f195-402d-d1b6-a5439478a77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "training_set =  []\n",
        "validation_set =  []\n",
        "testing_set =  []\n",
        "\n",
        "def transform_X(X):\n",
        "  X_transformed = X\n",
        "  #X_transformed = np.hstack((X_transformed, X ** 2))\n",
        "  #X_transformed = np.hstack((X_transformed, X ** 3))\n",
        "  X_transformed = np.column_stack((np.ones(X_transformed.shape[0]), X_transformed))\n",
        "  return X_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  global training_datalist, testing_datalist\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  spliting_index = int(len(training_datalist[1:])*0.9)\n",
        "  training_set = training_datalist[1:spliting_index].astype(float)\n",
        "  validation_set = training_datalist[spliting_index:].astype(float)\n",
        "  testing_set = testing_datalist[1:].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(threshold_z):\n",
        "  global training_set, validation_set\n",
        "  global training_datalist\n",
        "  traning_all_set = training_datalist[1:].astype(float)\n",
        "\n",
        "  # Use the Z Score method\n",
        "  mean_dbp = np.mean(traning_all_set[:, 0])\n",
        "  std_dbp = np.std(traning_all_set[:, 0])\n",
        "  mean_sbp = np.mean(traning_all_set[:, 1])\n",
        "  std_sbp = np.std(traning_all_set[:, 1])\n",
        "\n",
        "  # Preprocess the training set\n",
        "  z_scores_dbp = (training_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp = np.where(np.abs(z_scores_dbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_dbp, axis=0)\n",
        "\n",
        "  z_scores_sbp = (training_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp = np.where(np.abs(z_scores_sbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_sbp, axis=0)\n",
        "\n",
        "  ''' I should not preprocess the validation set\n",
        "  # Preprocess the validating set\n",
        "  z_scores_dbp_val = (validation_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp_val = np.where(np.abs(z_scores_dbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_dbp_val, axis=0)\n",
        "\n",
        "  z_scores_sbp_val = (validation_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp_val = np.where(np.abs(z_scores_sbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_sbp_val, axis=0)\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(X, Y):\n",
        "  X = transform_X(X)\n",
        "  W = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "\n",
        "  # Validate W with validation set\n",
        "  def mape(Y_true, Y_pred):\n",
        "    return np.mean(np.abs((Y_true - Y_pred) / Y_true))\n",
        "\n",
        "  '''\n",
        "  X_validation = np.array([row[0] for row in validation_set]).reshape(-1, 1)\n",
        "  X_validation = transform_X(X_validation)\n",
        "  predicted_sbp = X_validation @ W\n",
        "  actual_sbp_values = np.array([row[1] for row in validation_set])\n",
        "  print(\"MAPE from validation_datalist:\", mape(actual_sbp_values, predicted_sbp)*100, \" %\")\n",
        "  '''\n",
        "\n",
        "  return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(new_dbp_set, W):\n",
        "  global output_datalist\n",
        "  X_test = transform_X(new_dbp_set)\n",
        "  output_datalist = np.round(X_test @ W, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [],
      "source": [
        "SplitData()\n",
        "PreprocessData(2)\n",
        "\n",
        "X = training_set[:, 0].reshape(-1, 1)\n",
        "Y = training_set[:, 1].reshape(-1, 1)\n",
        "W = MatrixInversion(X, Y)\n",
        "\n",
        "print(' '.join(map(str, W[::-1].flatten())))\n",
        "\n",
        "# Start to prediction on the testing data\n",
        "dbp_testing_values = np.array([row[0] for row in testing_set]).reshape(-1, 1)\n",
        "MakePrediction(dbp_testing_values, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "training_set =  []\n",
        "validation_set =  []\n",
        "testing_set =  []\n",
        "highest_degree = 1\n",
        "\n",
        "def transform_X_and_W(X, highest_degree):\n",
        "  X_transformed = X\n",
        "\n",
        "  for i in range(highest_degree-1):\n",
        "    X_transformed = np.hstack((X_transformed, X ** (highest_degree+2)))\n",
        "  X_transformed = np.column_stack((np.ones(X_transformed.shape[0]), X_transformed))\n",
        "  W = np.zeros((highest_degree+1, 1)).astype(float)\n",
        "  return X_transformed, W\n",
        "\n",
        "# Open files\n",
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  global training_datalist, testing_datalist\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  spliting_index = int(len(training_datalist[1:])*0.9)\n",
        "  training_set = training_datalist[1:spliting_index].astype(float)\n",
        "  validation_set = training_datalist[spliting_index:].astype(float)\n",
        "  testing_set = testing_datalist[1:].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(threshold_z):\n",
        "  global training_set, validation_set\n",
        "  global training_datalist\n",
        "  traning_all_set = training_datalist[1:].astype(float)\n",
        "\n",
        "  # Use the Z Score method\n",
        "  mean_dbp = np.mean(traning_all_set[:, 0])\n",
        "  std_dbp = np.std(traning_all_set[:, 0])\n",
        "  mean_sbp = np.mean(traning_all_set[:, 1])\n",
        "  std_sbp = np.std(traning_all_set[:, 1])\n",
        "\n",
        "  # Preprocess the training set\n",
        "  z_scores_dbp = (training_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp = np.where(np.abs(z_scores_dbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_dbp, axis=0)\n",
        "\n",
        "  z_scores_sbp = (training_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp = np.where(np.abs(z_scores_sbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_sbp, axis=0)\n",
        "\n",
        "  ''' I should not preprocess the validation set\n",
        "  # Preprocess the validating set\n",
        "  z_scores_dbp_val = (validation_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp_val = np.where(np.abs(z_scores_dbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_dbp_val, axis=0)\n",
        "\n",
        "  z_scores_sbp_val = (validation_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp_val = np.where(np.abs(z_scores_sbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_sbp_val, axis=0)\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(X, Y, learning_rate=0.0001, target_mape=0.05, max_iterations=100000):\n",
        "  def mape(Y_true, Y_pred):\n",
        "    return np.mean(np.abs((Y_true - Y_pred) / Y_true))\n",
        "\n",
        "  '''\n",
        "  def loss_function(X, Y, W):\n",
        "    predictions = X @ W\n",
        "    squared_error = (predictions - Y) ** 2\n",
        "    return np.sum(squared_error) / (2 * len(Y)) # /(2*m) is for normalization\n",
        "  '''\n",
        "\n",
        "  global coefficient_output, validation_set, highest_degree\n",
        "  coefficient_output = []\n",
        "  X, W = transform_X_and_W(X, highest_degree)\n",
        "\n",
        "  iteration = 0\n",
        "  m = len(Y)\n",
        "\n",
        "  while iteration < max_iterations:\n",
        "    gradient = (X.T @ (X @ W - Y)) / m # m is for normalization as well\n",
        "    W = W - learning_rate * gradient\n",
        "\n",
        "    # Validate W with validation set\n",
        "    X_validation = np.array([row[0] for row in validation_set]).reshape(-1, 1)\n",
        "    X_validation, W_not_used = transform_X_and_W(X_validation, highest_degree)\n",
        "    predicted_sbp = X_validation @ W\n",
        "    actual_sbp_values = np.array([row[1] for row in validation_set])\n",
        "    current_mape = mape(actual_sbp_values, predicted_sbp)\n",
        "\n",
        "    coefficient_output.append(np.concatenate(W))\n",
        "\n",
        "    #print(f\"Iteration {iteration + 1}: MAPE from validation_set = {current_mape*100}%\")\n",
        "\n",
        "    if current_mape < target_mape:\n",
        "      break\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "  #print(f\"Final MAPE from validation_set after Iteration {iteration} = {current_mape*100}%\")\n",
        "  coefficient_output = np.array(coefficient_output)\n",
        "  return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(new_dbp_set, W):\n",
        "  global output_datalist, highest_degree\n",
        "  X_test, W_not_used = transform_X_and_W(new_dbp_set, highest_degree)\n",
        "  output_datalist = np.round(X_test @ W, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [],
      "source": [
        "SplitData()\n",
        "PreprocessData(2)\n",
        "\n",
        "X = training_set[:, 0].reshape(-1, 1)\n",
        "Y = training_set[:, 1].reshape(-1, 1)\n",
        "W = GradientDescent(X, Y, 0.00025, 0.1, 100000)\n",
        "\n",
        "print(' '.join(map(str, W[::-1].flatten())))\n",
        "\n",
        "# Start to prediction on the testing data\n",
        "dbp_testing_values = np.array([row[0] for row in testing_set]).reshape(-1, 1)\n",
        "MakePrediction(dbp_testing_values, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFecGOR4Np4t"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBI2qSkANp4t"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "6dlSor5LNp4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037ca14e-1a8b-484c-d5ec-9777189ef190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from validation_set: 10.22441265219449  %\n"
          ]
        }
      ],
      "source": [
        "training_set =  {}\n",
        "validation_set =  {}\n",
        "\n",
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "def CountAllDataFrame_FromDictionary(data_dict):\n",
        "  count = sum(len(data_df) for data_df in data_dict.values())\n",
        "  print(f\"Total DataFrames: {count}\")\n",
        "\n",
        "def PrintAllDataFrame_FromDictionary(data_dict):\n",
        "  for subject_id, data_df in training_set.items():\n",
        "    print(subject_id, \":\\n\", data_df)\n",
        "\n",
        "def ProcessTimeString_ToSeconds(time_str):\n",
        "  #'D days hh:mm:ss'\n",
        "  if len(time_str) == 0 or time_str == '0' or time_str == 0:\n",
        "    return ''\n",
        "\n",
        "  time_parts = time_str.split()\n",
        "  days = int(time_parts[0])\n",
        "  hh_mm_ss = time_parts[2]\n",
        "\n",
        "  hh_mm_ss_parts = hh_mm_ss.split(':')\n",
        "  hours = int(hh_mm_ss_parts[0])\n",
        "  minutes = int(hh_mm_ss_parts[1])\n",
        "  seconds = int(hh_mm_ss_parts[2])\n",
        "\n",
        "  total_seconds = (days * 24 * 60 * 60) + (hours * 60 * 60) + (minutes * 60) + seconds\n",
        "  return total_seconds\n",
        "\n",
        "def SplitData():\n",
        "  global training_datalist, testing_datalist\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  columns = ['subject_id', 'charttime', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']\n",
        "\n",
        "  # For training and validation\n",
        "  for i in range(len(training_datalist[1:])):\n",
        "    training_datalist[i+1, 1] = ProcessTimeString_ToSeconds(training_datalist[i+1, 1])\n",
        "\n",
        "  df = pd.DataFrame(training_datalist, columns=columns)\n",
        "  subject_data = {}\n",
        "  for subject_id, subject_df in df.groupby('subject_id'):\n",
        "    subject_data[subject_id] = subject_df.drop('subject_id', axis=1)\n",
        "\n",
        "  for subject_id, data_df in subject_data.items():\n",
        "    data_array = data_df.values\n",
        "    num_samples = len(data_array)\n",
        "    num_train_samples = int(0.9 * num_samples)  # 90% 用于训练，10% 用于验证\n",
        "    train_data = data_array[:num_train_samples, :]\n",
        "    valid_data = data_array[num_train_samples:, :]\n",
        "    training_set[subject_id] = pd.DataFrame(train_data, columns=columns[1:])\n",
        "    validation_set[subject_id] = pd.DataFrame(valid_data, columns=columns[1:])\n",
        "\n",
        "  if 'subject_id' in training_set:\n",
        "    del training_set['subject_id']\n",
        "  if 'subject_id' in validation_set:\n",
        "    del validation_set['subject_id']\n",
        "\n",
        "def PreprocessData(threshold = 2):\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  # All data type to float and clean data with empty block\n",
        "  columns_to_process = ['charttime', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']\n",
        "  for subject_id, train_data_df in training_set.items():\n",
        "    for column in columns_to_process:\n",
        "        train_data_df[column] = pd.to_numeric(train_data_df[column], errors='coerce')\n",
        "    train_data_df.dropna(subset=columns_to_process, inplace=True)\n",
        "  for subject_id, valid_data_df in validation_set.items():\n",
        "    for column in columns_to_process:\n",
        "        valid_data_df[column] = pd.to_numeric(valid_data_df[column], errors='coerce')\n",
        "    valid_data_df.dropna(subset=columns_to_process, inplace=True)\n",
        "\n",
        "  # testing data\n",
        "  for i in range(len(testing_datalist[1:])):\n",
        "    testing_datalist[i+1, 1] = ProcessTimeString_ToSeconds(testing_datalist[i+1, 1])\n",
        "\n",
        "  # Cleaning outliers\n",
        "  for key, df in training_set.items():\n",
        "    for column in df.columns:\n",
        "        if column != 'charttime':\n",
        "            z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
        "            outlier_indices = z_scores > threshold\n",
        "            df = df[~outlier_indices]\n",
        "    training_set[key] = df\n",
        "\n",
        "  for key, df in validation_set.items():\n",
        "    for column in df.columns:\n",
        "        if column != 'charttime':\n",
        "            z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
        "            outlier_indices = z_scores > threshold\n",
        "            df = df[~outlier_indices]\n",
        "    validation_set[key] = df\n",
        "\n",
        "def transform_dataframe_into_nparray(data_dict, selected_columns):\n",
        "  data_dict_transformed = {}\n",
        "  for dataset_name, df in data_dict.items():\n",
        "    data_dict_transformed[dataset_name] = df[selected_columns].values\n",
        "  return data_dict_transformed\n",
        "\n",
        "def MatrixInversion(X, Y):\n",
        "  W = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "  return W\n",
        "\n",
        "def validate_and_print_MAPE(W_dict):\n",
        "  global validation_set\n",
        "\n",
        "  MAPE_array = []\n",
        "  for dataset_name, df in validation_set.items():\n",
        "    for index, row in df.iterrows():\n",
        "      row = np.array(row.values).astype(float)\n",
        "      X_validation = row[:5]\n",
        "      Y_pred = X_validation @ W_dict[dataset_name]\n",
        "      Y_true = row[5:]\n",
        "      MAPE_num = (Y_true[0] - Y_pred[0]) / Y_true[0]\n",
        "      MAPE_array.append(MAPE_num)\n",
        "  print(\"MAPE from validation_set:\", np.mean(np.abs(MAPE_array))*100, \" %\")\n",
        "\n",
        "def MakePrediction(X_test, W):\n",
        "  return np.round(X_test @ W, 3)\n",
        "\n",
        "SplitData()\n",
        "PreprocessData(2)\n",
        "\n",
        "# Matrix Inversion\n",
        "X_dict = transform_dataframe_into_nparray(training_set, ['charttime', 'temperature', 'heartrate', 'resprate', 'o2sat'])\n",
        "Y_dict = transform_dataframe_into_nparray(training_set, ['sbp'])\n",
        "W_dict = {}\n",
        "for dataset_name in X_dict.keys():\n",
        "  X = X_dict[dataset_name]\n",
        "  Y = Y_dict[dataset_name]\n",
        "  W = MatrixInversion(X, Y)\n",
        "  #print(' '.join(map(str, W[::-1].flatten())))\n",
        "  W_dict[dataset_name] = W\n",
        "\n",
        "validate_and_print_MAPE(W_dict)\n",
        "\n",
        "# Start to prediction on the testing data\n",
        "for patient in testing_datalist[1:]:\n",
        "  subject_id = patient[0]\n",
        "  X_test = patient[1:6].astype(float)\n",
        "  W = W_dict[subject_id]\n",
        "  Y_predicted = MakePrediction(X_test, W)\n",
        "  output_datalist.append(Y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbXMQqenNp4t"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "i2JjBPVINp4u"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}