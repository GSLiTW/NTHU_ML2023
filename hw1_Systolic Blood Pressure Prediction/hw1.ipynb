{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V06iW39mNy01",
        "outputId": "5fe9bdb3-f195-402d-d1b6-a5439478a77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "training_set =  []\n",
        "validation_set =  []\n",
        "testing_set =  []\n",
        "\n",
        "def transform_X(X):\n",
        "  X_transformed = X\n",
        "  #X_transformed = np.hstack((X_transformed, X ** 2))\n",
        "  #X_transformed = np.hstack((X_transformed, X ** 3))\n",
        "  X_transformed = np.column_stack((np.ones(X_transformed.shape[0]), X_transformed))\n",
        "  return X_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  global training_datalist, testing_datalist\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  spliting_index = int(len(training_datalist[1:])*0.9)\n",
        "  training_set = training_datalist[1:spliting_index].astype(float)\n",
        "  validation_set = training_datalist[spliting_index:].astype(float)\n",
        "  testing_set = testing_datalist[1:].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(threshold_z):\n",
        "  global training_set, validation_set\n",
        "  global training_datalist\n",
        "  traning_all_set = training_datalist[1:].astype(float)\n",
        "\n",
        "  # Use the Z Score method\n",
        "  mean_dbp = np.mean(traning_all_set[:, 0])\n",
        "  std_dbp = np.std(traning_all_set[:, 0])\n",
        "  mean_sbp = np.mean(traning_all_set[:, 1])\n",
        "  std_sbp = np.std(traning_all_set[:, 1])\n",
        "\n",
        "  # Preprocess the training set\n",
        "  z_scores_dbp = (training_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp = np.where(np.abs(z_scores_dbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_dbp, axis=0)\n",
        "\n",
        "  z_scores_sbp = (training_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp = np.where(np.abs(z_scores_sbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_sbp, axis=0)\n",
        "\n",
        "  # Preprocess the validating set\n",
        "  z_scores_dbp_val = (validation_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp_val = np.where(np.abs(z_scores_dbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_dbp_val, axis=0)\n",
        "\n",
        "  z_scores_sbp_val = (validation_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp_val = np.where(np.abs(z_scores_sbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_sbp_val, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(X, Y):\n",
        "  #print(training_set)\n",
        "\n",
        "  X = transform_X(X)\n",
        "  W = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "\n",
        "  # Validate W with validation set\n",
        "  def mape(Y_true, Y_pred):\n",
        "    return np.mean(np.abs((Y_true - Y_pred) / Y_true))\n",
        "\n",
        "  #'''\n",
        "  X_validation = np.array([row[0] for row in validation_set]).reshape(-1, 1)\n",
        "  X_validation = transform_X(X_validation)\n",
        "  predicted_sbp = X_validation @ W\n",
        "  actual_sbp_values = np.array([row[1] for row in validation_set])\n",
        "  print(\"MAPE from validation_datalist:\", mape(actual_sbp_values, predicted_sbp)*100, \" %\")\n",
        "  #'''\n",
        "\n",
        "  return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(new_dbp_set, W):\n",
        "  global output_datalist\n",
        "  X_test = transform_X(new_dbp_set)\n",
        "  output_datalist = np.round(X_test @ W, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "iCL92EPKOFIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3b9b72-9664-4864-f42e-f3caa45c0640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from validation_datalist: 2.6405952133528263  %\n",
            "0.28034701442542137 107.71589031306922\n"
          ]
        }
      ],
      "source": [
        "SplitData()\n",
        "PreprocessData(0.2)\n",
        "\n",
        "X = training_set[:, 0].reshape(-1, 1)\n",
        "Y = training_set[:, 1].reshape(-1, 1)\n",
        "W = MatrixInversion(X, Y)\n",
        "\n",
        "print(' '.join(map(str, W[::-1].flatten())))\n",
        "\n",
        "# Start to prediction on the testing data\n",
        "dbp_testing_values = np.array([row[0] for row in testing_set]).reshape(-1, 1)\n",
        "MakePrediction(dbp_testing_values, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "training_set =  []\n",
        "validation_set =  []\n",
        "testing_set =  []\n",
        "highest_degree = 1\n",
        "\n",
        "def transform_X_and_W(X, highest_degree):\n",
        "  X_transformed = X\n",
        "\n",
        "  for i in range(highest_degree-1):\n",
        "    X_transformed = np.hstack((X_transformed, X ** (highest_degree+2)))\n",
        "  X_transformed = np.column_stack((np.ones(X_transformed.shape[0]), X_transformed))\n",
        "  W = np.zeros((highest_degree+1, 1)).astype(float)\n",
        "  return X_transformed, W\n",
        "\n",
        "# Open files\n",
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  global training_datalist, testing_datalist\n",
        "  global training_set, validation_set, testing_set\n",
        "\n",
        "  spliting_index = int(len(training_datalist[1:])*0.9)\n",
        "  training_set = training_datalist[1:spliting_index].astype(float)\n",
        "  validation_set = training_datalist[spliting_index:].astype(float)\n",
        "  testing_set = testing_datalist[1:].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(threshold_z):\n",
        "  global training_set, validation_set\n",
        "  global training_datalist\n",
        "  traning_all_set = training_datalist[1:].astype(float)\n",
        "\n",
        "  # Use the Z Score method\n",
        "  mean_dbp = np.mean(traning_all_set[:, 0])\n",
        "  std_dbp = np.std(traning_all_set[:, 0])\n",
        "  mean_sbp = np.mean(traning_all_set[:, 1])\n",
        "  std_sbp = np.std(traning_all_set[:, 1])\n",
        "\n",
        "  # Preprocess the training set\n",
        "  z_scores_dbp = (training_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp = np.where(np.abs(z_scores_dbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_dbp, axis=0)\n",
        "\n",
        "  z_scores_sbp = (training_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp = np.where(np.abs(z_scores_sbp) > threshold_z)\n",
        "  training_set = np.delete(training_set, outliers_sbp, axis=0)\n",
        "\n",
        "  # Preprocess the validating set\n",
        "  z_scores_dbp_val = (validation_set[:, 0] - mean_dbp) / std_dbp\n",
        "  outliers_dbp_val = np.where(np.abs(z_scores_dbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_dbp_val, axis=0)\n",
        "\n",
        "  z_scores_sbp_val = (validation_set[:, 1] - mean_sbp) / std_sbp\n",
        "  outliers_sbp_val = np.where(np.abs(z_scores_sbp_val) > threshold_z)\n",
        "  validation_set = np.delete(validation_set, outliers_sbp_val, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(X, Y, learning_rate=0.0001, target_mape=0.05, max_iterations=100000):\n",
        "  def mape(Y_true, Y_pred):\n",
        "    return np.mean(np.abs((Y_true - Y_pred) / Y_true))\n",
        "\n",
        "  '''\n",
        "  def loss_function(X, Y, W):\n",
        "    predictions = X @ W\n",
        "    squared_error = (predictions - Y) ** 2\n",
        "    return np.sum(squared_error) / (2 * len(Y)) # /(2*m) is for normalization\n",
        "  '''\n",
        "\n",
        "  global coefficient_output, validation_set, highest_degree\n",
        "  coefficient_output = []\n",
        "  X, W = transform_X_and_W(X, highest_degree)\n",
        "\n",
        "  iteration = 0\n",
        "  m = len(Y)\n",
        "\n",
        "  while iteration < max_iterations:\n",
        "    gradient = (X.T @ (X @ W - Y)) / m # m is for normalization as well\n",
        "    W = W - learning_rate * gradient\n",
        "\n",
        "    # Validate W with validation set\n",
        "    X_validation = np.array([row[0] for row in validation_set]).reshape(-1, 1)\n",
        "    X_validation, W_not_used = transform_X_and_W(X_validation, highest_degree)\n",
        "    predicted_sbp = X_validation @ W\n",
        "    actual_sbp_values = np.array([row[1] for row in validation_set])\n",
        "    current_mape = mape(actual_sbp_values, predicted_sbp)\n",
        "\n",
        "    coefficient_output.append(np.concatenate(W))\n",
        "\n",
        "    #print(f\"Iteration {iteration + 1}: MAPE from validation_set = {current_mape*100}%\")\n",
        "\n",
        "    if current_mape < target_mape:\n",
        "      break\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "  #print(f\"Final MAPE from validation_set after Iteration {iteration} = {current_mape*100}%\")\n",
        "  coefficient_output = np.array(coefficient_output)\n",
        "  return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(new_dbp_set, W):\n",
        "  global output_datalist, highest_degree\n",
        "  X_test, W_not_used = transform_X_and_W(new_dbp_set, highest_degree)\n",
        "  output_datalist = np.round(X_test @ W, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "90EisOc7YG-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a9a8a3-f3a9-4edf-ab35-7c309e1034ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final MAPE from validation_set after Iteration 100000 = 3.7299858379051796%\n",
            "1.5058862871203298 4.0580212996334355\n"
          ]
        }
      ],
      "source": [
        "SplitData()\n",
        "PreprocessData(0.2)\n",
        "\n",
        "X = training_set[:, 0].reshape(-1, 1)\n",
        "Y = training_set[:, 1].reshape(-1, 1)\n",
        "W = GradientDescent(X, Y, 0.0002, 0.005, 100000)\n",
        "\n",
        "print(' '.join(map(str, W[::-1].flatten())))\n",
        "\n",
        "# Start to prediction on the testing data\n",
        "dbp_testing_values = np.array([row[0] for row in testing_set]).reshape(-1, 1)\n",
        "MakePrediction(dbp_testing_values, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFecGOR4Np4t"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBI2qSkANp4t"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dlSor5LNp4t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbXMQqenNp4t"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2JjBPVINp4u"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}